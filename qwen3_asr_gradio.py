import os
import sys
import asyncio

# ==========================================
# Windows ç¯å¢ƒä¿®å¤ (å¿…é¡»åœ¨å…¶ä»–å¯¼å…¥å‰æ‰§è¡Œ)
# ==========================================
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# è®¾ç½®é»˜è®¤ç¼–ç ä¸º UTF-8
import locale
try:
    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
except locale.Error:
    pass
os.environ['PYTHONIOENCODING'] = 'utf-8'

# ==========================================
# å¯¼å…¥æ¨¡å—
# ==========================================
import argparse
import gradio as gr
from loguru import logger
from qwen3_asr_handler import (
    Qwen3ASRHandler,
    scan_local_models,
    GLOBAL_VRAM_CONFIG,
    auto_detect_precision,
    auto_detect_flash_attention,
    get_optimal_chunk_size
)

# ==========================================
# å…¨å±€ Handler å®ä¾‹
# ==========================================
handler = None

def initialize_service(model_dir, model_name, device, precision, quantization, compile_model, flash_attention):
    """Initialize the ASR service"""
    global handler
    if quantization != "none" and compile_model:
        logger.warning("Quantization is enabled. Disabling torch.compile to improve stability and load time.")
        compile_model = False
    
    handler = Qwen3ASRHandler(model_dir)
    status, success = handler.initialize(
        model_name=model_name,
        device=device,
        precision=precision,
        quantization=quantization,
        compile_model=compile_model,
        flash_attention=flash_attention
    )
    return status

def transcribe_audio(audio_file, language, chunk_size, overlap):
    """Transcribe uploaded audio file"""
    global handler
    if handler is None or handler.model is None:
        return "", "âŒ Service not initialized. Please initialize first."
    if audio_file is None:
        return "", "âŒ No audio file uploaded."
    try:
        import soundfile as sf
        waveform, sr = sf.read(audio_file)
        
        if len(waveform.shape) > 1:
            waveform = waveform.mean(axis=1)
        
        text, timestamps = handler.transcribe(
            audio_waveform=waveform,
            sample_rate=sr,
            language=language,
            chunk_size=chunk_size,
            overlap=overlap
        )
        
        return text, timestamps if timestamps else "Transcription complete."
    except Exception as e:
        logger.exception("Transcription failed")
        return "", f"âŒ Error: {str(e)}"

def create_ui():
    """Create Gradio Interface"""
    with gr.Blocks(title="Qwen3-ASR") as demo:
        gr.Markdown("# ğŸ¤ Qwen3-ASR è¯­éŸ³è¯†åˆ«")
        gr.Markdown(f"### ğŸ–¥ï¸ GPU: {GLOBAL_VRAM_CONFIG.tier} ({GLOBAL_VRAM_CONFIG.gpu_memory_gb:.1f} GB)")
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("### âš™ï¸ é…ç½®")
                
                # æ¨¡å‹ç›®å½•
                model_dir_input = gr.Textbox(
                    label="æ¨¡å‹ç›®å½•",
                    value="./models",
                    info="å­˜æ”¾æ¨¡å‹æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„"
                )
                
                # åˆ·æ–°æ¨¡å‹åˆ—è¡¨
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æ¨¡å‹")
                model_list = scan_local_models(model_dir_input.value)
                model_dropdown = gr.Dropdown(
                    choices=model_list,
                    label="é€‰æ‹©æ¨¡å‹",
                    value=model_list[0] if model_list else None
                )
                
                # è®¾å¤‡é…ç½®
                device_dropdown = gr.Dropdown(
                    choices=["auto", "cuda", "cpu"],
                    value="auto",
                    label="è¿è¡Œè®¾å¤‡"
                )
                
                precision_dropdown = gr.Dropdown(
                    choices=["auto", "bf16", "fp16", "fp32"],
                    value="auto",
                    label="è®¡ç®—ç²¾åº¦"
                )
                
                quant_dropdown = gr.Dropdown(
                    choices=["none", "int8_weight_only", "fp8_weight_only"],
                    value="int8_weight_only",
                    label="é‡åŒ– (TorchAO)"
                )
                
                compile_check = gr.Checkbox(
                    label="å¯ç”¨ torch.compile (é‡åŒ–æ—¶ä¸å»ºè®®å¯ç”¨)", 
                    value=False
                )
                flash_check = gr.Checkbox(
                    label="å¯ç”¨ Flash Attention", 
                    value=auto_detect_flash_attention()
                )
                
                init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–æ¨¡å‹", variant="primary")
                init_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
            
            with gr.Column():
                gr.Markdown("### ğŸµ è½¬å½•")
                audio_input = gr.Audio(type="filepath", label="ä¸Šä¼ éŸ³é¢‘")
                lang_dropdown = gr.Dropdown(
                    choices=["auto", "Chinese", "English", "Japanese", "Korean"],
                    value="auto",
                    label="è¯­è¨€"
                )
                
                recommended_chunk = get_optimal_chunk_size()
                chunk_slider = gr.Slider(
                    0, 300, 
                    value=recommended_chunk, 
                    step=5, 
                    label=f"åˆ†å—å¤§å° (ç§’ï¼Œ0=è‡ªåŠ¨ï¼Œæ¨èï¼š{recommended_chunk}s)"
                )
                overlap_slider = gr.Slider(0, 10, value=2, step=1, label="é‡å  (ç§’)")
                
                transcribe_btn = gr.Button("ğŸ“ å¼€å§‹è½¬å½•", variant="primary")
                text_output = gr.Textbox(label="è½¬å½•ç»“æœ", lines=10)
                timestamp_output = gr.Textbox(label="ä¿¡æ¯", lines=5)
        
        # ==========================================
        # äº‹ä»¶ç»‘å®š
        # ==========================================
        def refresh_models(dir):
            models = scan_local_models(dir)
            return gr.Dropdown(choices=models, value=models[0] if models else None)
        
        refresh_btn.click(fn=refresh_models, inputs=[model_dir_input], outputs=[model_dropdown])
        
        init_btn.click(
            fn=initialize_service,
            inputs=[model_dir_input, model_dropdown, device_dropdown, precision_dropdown, quant_dropdown, compile_check, flash_check],
            outputs=[init_status]
        )
        
        transcribe_btn.click(
            fn=transcribe_audio,
            inputs=[audio_input, lang_dropdown, chunk_slider, overlap_slider],
            outputs=[text_output, timestamp_output]
        )

    return demo

def main():
    parser = argparse.ArgumentParser(description="Qwen3-ASR Gradio App")
    parser.add_argument("--port", type=int, default=7860, help="Gradio ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument(
        "--model_dir",
        type=str,
        default="./models",
        help="é»˜è®¤æ¨¡å‹ç›®å½•"
    )
    args = parser.parse_args()
    
    logger.info(f"GPU: {GLOBAL_VRAM_CONFIG.tier} ({GLOBAL_VRAM_CONFIG.gpu_memory_gb:.1f} GB)")
    logger.info(f"é»˜è®¤ç²¾åº¦ï¼š{auto_detect_precision()}")
    logger.info(f"Flash Attention: {auto_detect_flash_attention()}")
    logger.info(f"æ¨èåˆ†å—å¤§å°ï¼š{get_optimal_chunk_size()}s")

    demo = create_ui()
    demo.launch(
        server_name="127.0.0.1", 
        server_port=args.port, 
        share=args.share,
        inbrowser=True
    )

if __name__ == "__main__":
    main()